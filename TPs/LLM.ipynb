{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1f2a3a-5684-4b8d-9816-d64266b378f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Encodage de Texte par les LLM\n",
    "\n",
    "L'objectif de ce notebook est montrer des exemples de d'encodage de textes et de calcul de similarité. La partie tokenizer rappelle quelques spécifictés des toknizers, i.e. le type de textes qu'ils encodent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a9f032-5ce6-4e41-99bb-1287e8a8a922",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bougha/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d4a46-32a5-41a2-b51f-6c74a0f8857c",
   "metadata": {
    "tags": []
   },
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ba4d4-c761-443e-b41c-7eed9b399131",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1- Tokenizer : Exemples de tokenizers\n",
    "Le tokenizer des modèles LLMs sont spéciaux. Ils ne traitent pas les mots comme on l'a vu dans les chapitres relatifs à la représentation des textes, mais des sous-mots (token).\n",
    "Les cellules ci-dessous présentent des exemples de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df58e091-1936-43e7-87af-3d3168599ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_tokens(text,tokenizer_name):\n",
    "    tokenizer=AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    tokens = tokenizer(text)\n",
    "    token_ids = tokens.input_ids  \n",
    "    print(\"tokens_decoded:\", \" ; \".join([tokenizer.decode([t]) for t in token_ids]))\n",
    "    print(token_ids )\n",
    "    #print(\"\\n\") \n",
    "def encode_decode(text, tokenizer_name):\n",
    "    tokenizer=AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    token_ids = tokenizer(text).input_ids   \n",
    "    print(tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b061cddb-cb8d-44c0-874c-0e818615eded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "English and CAPITALIZATION\n",
    "):  ♫  􀀀\n",
    "show _tokens False None elif == >= else :\n",
    "Four spaces : \"     \" Two tabs : \"  \"\n",
    "12 . 0 * 50 = 600\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb7adc5-17b9-4f71-82ab-ea8ccd86967d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_decoded: [CLS] ; english ; and ; capital ; ##ization ; ) ; : ; [UNK] ; show ; _ ; token ; ##s ; false ; none ; eli ; ##f ; = ; = ; > ; = ; else ; : ; four ; spaces ; : ; \" ; \" ; two ; tab ; ##s ; : ; \" ; \" ; 12 ; . ; 0 ; * ; 50 ; = ; 600 ; [SEP]\n",
      "[101, 2394, 1998, 3007, 3989, 1007, 1024, 100, 2265, 1035, 19204, 2015, 6270, 3904, 12005, 2546, 1027, 1027, 1028, 1027, 2842, 1024, 2176, 7258, 1024, 1000, 1000, 2048, 21628, 2015, 1024, 1000, 1000, 2260, 1012, 1014, 1008, 2753, 1027, 5174, 102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bougha/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "show_tokens(text,\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f889394d-3c1e-49a8-8d3f-97a88eac9e30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encode_decode(text,\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a81c32f-ee46-40ac-bf1f-d5b68dc5d363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_tokens(text,\"gpt2\")\n",
    "encode_decode(text,\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730323e-418f-4878-ab11-2a6ea6cf2a82",
   "metadata": {
    "id": "0G75OPxmbSSu",
    "tags": []
   },
   "source": [
    "## 2- Encodage de texte avec des transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eda84d-c3fa-45dc-85e7-5ee38df89e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AutoModel, AutoTokenizer\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "292842f1-c47a-48bb-a5ba-8e696f843454",
   "metadata": {
    "id": "kcjWFbWCzCjp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choisir une option lecture du Texte et la procédure de construction des tockens\n",
    "\n",
    "# Sample documents\n",
    "def get_documents():\n",
    "    documents = [\n",
    "    \"Text retrieval is the process of finding documents that are relevant to a user's query.\",    \n",
    "    \"Gensim is a popular library for topic modeling and text retrieval in Python.\",\n",
    "    \"Cosine similarity is a metric used to measure how similar two vectors are.\",\n",
    "    \"Vectorization is the process of converting text data into numerical vectors.\",\n",
    "    \"Python is a versatile programming language used for various applications.\",\n",
    "    \"BM25 model is one of the main IR model,\"\n",
    "    ]  \n",
    "    return documents\n",
    "\n",
    "## Plusieurs fichiers dans un répértoire\n",
    "def readfiles_from_dir(dir_path='./data'):\n",
    "    for file_name in os.listdir(dir_path):\n",
    "        if \".txt\" in file_name:\n",
    "            texts = [simple_preprocess(remove_stopwords(sentence))\n",
    "                  for sentence in open(os.path.join(dir_path, file_name), encoding='utf-8')]\n",
    "    return texts\n",
    "\n",
    "# Function to read texts from a file\n",
    "def read_texts_from_file(file_path,n):\n",
    "    # lire les n lignes d'un csv\n",
    "    texts=pd.read_csv(file_path, encoding='utf-8',sep = '\\t', header=None, nrows=n)\n",
    "    #return uniquement la colonne 1 coortant le texte à traiter \n",
    "    return texts[1].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fc7a1b6-2020-4a41-8e4c-114ba005f826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lecture du fichier \n",
    "#file_path = './data/msmarco/collection.tsv'\n",
    "#documents = read_texts_from_file(file_path,100)\n",
    "#documents=documents\n",
    "#.astype(str)\n",
    "documents=get_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8dc1e-4e48-4bbc-bb35-74377c79bba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4469e5b-81c4-428f-8250-93fba13b7d62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_text(texts, tokenizer,model, max_length=512):\n",
    "    # Tokenize and encode the batch of texts\n",
    "    tokens = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    # Build the embeddings\n",
    "    outputs = model(**tokens)\n",
    "    # Get the last hidden state \n",
    "    embeddings = outputs.last_hidden_state\n",
    "    return get_cls(embeddings)\n",
    "\n",
    "# Get the CLS embeddings\n",
    "def get_cls(embeddings):\n",
    "    return embeddings[:, 0, :]\n",
    "    \n",
    "\n",
    "# Compute the mean of the Embeddings\n",
    "def get_mean(embeddings):\n",
    "    return(embeddings.mean(dim=0))\n",
    "\n",
    "# Extract the CLS token output (first token in the sequence)\n",
    "#cls_output = last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "\n",
    "# Display the CLS output\n",
    "#print(\"CLS Output Shape:\", cls_output.shape)  # Example: torch.Size([1, 768])\n",
    "#print(\"CLS Output Vector:\", cls_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59a6a38a-447f-4eb3-ad4a-0595d8aca313",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Batch Shape: torch.Size([6, 768])\n",
      "Encoded Batch Representations:\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "from transformers import AutoModel\n",
    "model_name = \"bert-base-uncased\"\n",
    "#model_name =\"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "# Encode the batch of texts\n",
    "doc_embeddings = encode_text(documents,tokenizer,model)\n",
    "\n",
    "# Display the encoded representations\n",
    "print(\"Encoded Batch Shape:\", doc_embeddings.shape)\n",
    "print(\"Encoded Batch Representations:\")\n",
    "#print(doc_embeddings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d559e-359c-4115-8741-d9cac01326f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcffbd88-f6ec-4cea-ac7c-dbf89366a707",
   "metadata": {},
   "source": [
    "## 3- Calcul de la similarité quer-text représentées sous forme d'embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e1e38b-9fcd-46e6-9ae0-708a4755bba4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked List of Documents:\n",
      "Document 6: BM25 model is one of the main IR model, Similarity: tensor(0.8291, grad_fn=<SelectBackward0>)\n",
      "Document 2: Gensim is a popular library for topic modeling and text retrieval in Python. Similarity: tensor(0.7916, grad_fn=<SelectBackward0>)\n",
      "Document 5: Python is a versatile programming language used for various applications. Similarity: tensor(0.7643, grad_fn=<SelectBackward0>)\n",
      "Document 4: Vectorization is the process of converting text data into numerical vectors. Similarity: tensor(0.6679, grad_fn=<SelectBackward0>)\n",
      "Document 1: Text retrieval is the process of finding documents that are relevant to a user's query. Similarity: tensor(0.6379, grad_fn=<SelectBackward0>)\n",
      "Document 3: Cosine similarity is a metric used to measure how similar two vectors are. Similarity: tensor(0.4853, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "# Encode query\n",
    "k=6\n",
    "query = \"bm25 model.\"\n",
    "query_embedding = encode_text(query, tokenizer, model)\n",
    "\n",
    "# Compute cosine similarity between query and documents\n",
    "similarities = cosine_similarity(query_embedding,doc_embeddings, dim=1)\n",
    "#reshape(-1, 1)\n",
    "\n",
    "#If dim=1, cosine similarity is computed for each row of the tensors.\n",
    "#If dim=0, cosine similarity is computed for each column of the tensors.\n",
    "    \n",
    "# Rank the documents based on similarity scores\n",
    "ranked_documents = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "# get les indices\n",
    "top_k_indices = [index for index, _ in ranked_documents[:k]]\n",
    "\n",
    "# Display the ranked list of documents\n",
    "print(\"Ranked List of Documents:\")\n",
    "for idx in top_k_indices:\n",
    "    print(f\"Document {idx + 1}: {documents[idx]}\", \"Similarity:\", similarities[idx])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b75467-f98a-44ff-a2c8-ea898517ebee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_embedding.shape\n",
    "doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2aaf65-6476-4e01-b615-c43103949ba0",
   "metadata": {},
   "source": [
    "## 4- Encodage d'un texte avec Sentence BERT model\n",
    "#### Modèle prêt à l'emploi, regardez sa care d'identité sur Jugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f816dd77-7711-4b74-81f8-0d178b417515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer,util\n",
    "model_sentence = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "038fb8b8-df77-4fcb-b05f-a5b8d3ac77a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_embeddings = model_sentence.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "191cdf43-3d70-4606-a912-07d0d35bddb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked List of Documents:\n",
      "Document 1: Text retrieval is the process of finding documents that are relevant to a user's query. Similarity: tensor([0.5131])\n",
      "Document 4: Vectorization is the process of converting text data into numerical vectors. Similarity: tensor([0.2193])\n",
      "Document 3: Cosine similarity is a metric used to measure how similar two vectors are. Similarity: tensor([0.1479])\n",
      "Document 6: BM25 model is one of the main IR model, Similarity: tensor([0.1276])\n",
      "Document 2: Gensim is a popular library for topic modeling and text retrieval in Python. Similarity: tensor([0.1183])\n"
     ]
    }
   ],
   "source": [
    "# Encode query\n",
    "k=5\n",
    "query = \"This is the query text.\"\n",
    "query_embedding = model_sentence.encode(query)\n",
    "\n",
    "# Compute cosine similarity between query and documents\n",
    "similarities = util.cos_sim(query_embedding,doc_embeddings).reshape(-1, 1)\n",
    "\n",
    "#similarities = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "# Rank the documents based on similarity scores\n",
    "ranked_documents = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# get les indices\n",
    "top_k_indices = [index for index, _ in ranked_documents[:k]]\n",
    "\n",
    "# Display the ranked list of documents\n",
    "print(\"Ranked List of Documents:\")\n",
    "for idx in top_k_indices:\n",
    "    print(f\"Document {idx + 1}: {documents[idx]}\", \"Similarity:\", similarities[idx])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8262d-7031-4027-a4c0-fde40e2ce8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch Env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
