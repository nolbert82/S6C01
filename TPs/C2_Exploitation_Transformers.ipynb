{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie II : Mod√®le (sch√©ma) commun d'utilisation des Hugging Face Transformers\n",
    "\n",
    "Ce Notebook montre un sch√©ma \"classique d'utilisation des Transformers, en utilisant l'exemple de l'analyse de sentiment.\n",
    "\n",
    "Tout d'abord, trouvez un mod√®le sur [the hub] (https://huggingface.co/models). Tout le monde peut t√©l√©charger son mod√®le pour que d'autres personnes puissent l'utiliser. \n",
    "\n",
    "Ensuite, deux objets doivent √™tre initialis√©s - un **tokenizer** et un **model**\n",
    "\n",
    "* Le tokenizer convertit les cha√Ænes de caract√®res en listes d'identifiants de vocabulaire dont le mod√®le a besoin.\n",
    "* Le mod√®le prend les identifiants de vocabulaire et produit une pr√©diction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installez la biblioth√®que ü§ó *Transformers* pour ex√©cuter ce *notebook*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "#[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Rappel pipelines \n",
    "Dans la premi√®re partie de ce Notebook nous avons utilis√© un pipeline end-to-end (une ligne de code).\n",
    "Ce pipeline est en fait compos√© de trois √©tapes (voir ci dessous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end-to-end pipeline.\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"tblard/tf-allocine\")\n",
    "classifier(\n",
    "    [\"J'ai attendu un cours d'HuggingFace toute ma vie.\",\n",
    "     \"Je d√©teste tellement √ßa !\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sch√©ma commun utilisation Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce pipeline regroupe trois √©tapes : le pr√©traitement (le **`tokenizer`**), le passage des entr√©es dans le **`mod√®le`** et le **`post-traitement`**.\n",
    "<figure>\n",
    "    <img src=\"./images/full_nlp_pipeline.svg\"  style=\"width:6000px;height:250px;\" >\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Etape 1 : Pr√©traitement - Tokenizer\n",
    "Les transformers ne peuvent pas traiter directement le texte brut, donc la premi√®re √©tape de notre pipeline est de convertir les entr√©es textuelles en identifiants (id) afin que le mod√®le puisse les comprendre. Pour ce faire, nous utilisons un tokenizer, qui sera responsable de :\n",
    "- diviser l‚Äôentr√©e en mots, sous-mots, ou symboles (comme la ponctuation) qui sont appel√©s tokens,\n",
    "- associer chaque token √† un nombre entier,\n",
    "- ajouter des entr√©es suppl√©mentaires qui peuvent √™tre utiles au mod√®le (`padding`).\n",
    "\n",
    "Key features:\n",
    "- Supports different tokenization techniques like Byte-Pair Encoding (BPE), WordPiece,\n",
    "and SentencePiece.\n",
    "- Tokenization happens quickly with parallelization support.\n",
    "- Handles special tokens like [CLS], [SEP], and padding/truncation automatically.\n",
    "- Easily load pre-trained tokenizers with AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "checkpoint = \"distilbert/distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    # J'ai attendu un cours de HuggingFace toute ma vie.\n",
    "    \"I hate this so much!\",  # Je d√©teste tellement √ßa !\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut appelerles tokenizers de diff√©rentes mani√®res. \n",
    "Soit en pr√©cisant le nom du mod√®le pr√©-entrain√©, par exemple `BertTokenizer`, soit en utilisant la classe `AutoTokenizer` qui r√©cup√®re la classe de `tokenizer` appropri√©e dans la biblioth√®que bas√©e sur le nom du checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert/distilbert-base-cased\")  \n",
    "tokenizer=AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    \n",
    "print (tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print (tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens_inputs=tokenizer(\"Using a Transformer network is simple\")\n",
    "\n",
    "tokens_inputs=tokenizer(\"Using a Transformer network is simple\",return_tensors=\"pt\")\n",
    "\n",
    "print(\"Vanilla Tokenization\")\n",
    "print(tokens_inputs)\n",
    "print()\n",
    "\n",
    "# Two ways to access:\n",
    "print(tokens_inputs.input_ids)\n",
    "print(tokens_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe `tokenizer ()` peut √™tre utilis√©e directement, elle s'occupe d'ffectuer toues les op√©raions n√©cessaires pour la construction d'une entr√©e ad√©quate au tranformer. Mais, on peut aussi d√©clider les diff√©rentes op√©rations.  \n",
    "\n",
    "| M√©thode                                       | Retourne                                       | Sp√©cificit√©s                                                           |       \n",
    "|----------------------------------------------|-----------------------------------------------|------------------------------------------------------------------------|\n",
    "| `tokenizer(text)`                            | `Dict` (`input_ids`, `attention_mask`, ...)   | Ajoute les **tokens sp√©ciaux** ; supporte `return_tensors=\"pt\"`        |\n",
    "| `tokenizer.encode(text)`                     | `List[int]` (IDs des tokens)                  | Peut ajouter les tokens sp√©ciaux (`add_special_tokens=True`)          | \n",
    "| `tokenizer.encode_plus(text)`                | `Dict` comme `tokenizer(text)`                | Permet `text_pair`, padding, truncation                              | \n",
    "| `tokenizer.batch_encode_plus([text1, ...])`  | `List[Dict]`                                  | Pour plusieurs textes (batch), avec padding et autres options         | \n",
    "| `tokenizer.tokenize(text)`                   | `List[str]` (tokens lisibles)                 | Affiche les sous-mots BPE / SentencePiece                             | \n",
    "| `tokenizer.convert_tokens_to_ids(tokens)`    | `List[int]`                                   | Convertit tokens ‚Üí IDs                                                | \n",
    "| `tokenizer.convert_ids_to_tokens(ids)`       | `List[str]`                                   | Convertit IDs ‚Üí tokens lisibles                                       | \n",
    "| `tokenizer.decode(ids)`                      | `str` (texte lisible)                         | Recompose le texte complet depuis les IDs                             | \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_str=\"Using a Transformer network is simple\"\n",
    "print(\"List tokens ID :\", tokenizer.encode(inputs_str))\n",
    "print(\"List tokens complets:\", tokenizer.encode_plus(inputs_str))\n",
    "\n",
    "tokens=tokenizer.tokenize(inputs_str)\n",
    "print(\"List tokens  :\", tokens)\n",
    "\n",
    "token_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Liste  des tokens Ids :\",token_ids)\n",
    "\n",
    "print(\"Id to tokens :\", tokenizer.convert_ids_to_tokens(token_ids))\n",
    "print(\"Id to texte originel :\", tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traiter plusieurs s√©quences de longueurs diff√©rentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut faire passer plusieurs s√©quences. \n",
    "#si on souhaite r√©cup√©rer un tenseur on doit utiliser un pading \n",
    "#pour que les vecteurs de chaque s√©quence (texte) ait la m√™me dimension.\n",
    "# teste avec et sans padding ?\n",
    "\n",
    "# Si tokenizer de GPT2 :  Set a padding token (use the EOS token for GPT-2)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_inputs = tokenizer([\"Hugging Face Transformers is great!\",\n",
    "                         \"The quick brown fox jumps over the lazy dog.\",\n",
    "                         \"Then the dog got up and ran away because she didn't like foxes.\",\n",
    "                         ],\n",
    "                         return_tensors=\"pt\", #tf : si tensorflow, ou np: Numpy\n",
    "                         padding=True,\n",
    "                         truncation=True\n",
    "                        )\n",
    "print(f\"Pad token: {tokenizer.pad_token} | Pad token id: {tokenizer.pad_token_id}\")\n",
    "print(\"Padding:\")\n",
    "print(model_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 2 : Passage au mod√®le- Choix du mod√®le de Transformer\n",
    "Il existe de nombreuses architectures diff√©rentes disponibles dans la biblioth√®que ü§ó Transformers, chacune √©tant con√ßue pour prendre en charge d‚Äôune t√¢che sp√©cifique. En voici une liste non exhaustive :\n",
    "\n",
    "- *Model (r√©cup√©rer les √©tats cach√©s : mod√®le de base)\n",
    "- *ForCausalLM\n",
    "- *ForMaskedLM\n",
    "- *ForMultipleChoice\n",
    "- *ForQuestionAnswering\n",
    "- *ForSequenceClassification\n",
    "- *ForTokenClassification\n",
    "- et autres ü§ó\n",
    "  \n",
    " `*` peut √™tre  `Auto` (par exemple `AutoModel`),  ou un mod√®le sp√©cifique (e.g. `Distil`Bert)\n",
    "\n",
    "Il existe 3 classes de mod√®les \n",
    "- Encoders (e.g. BERT)\n",
    "- Decoders (e.g. GPT2)\n",
    "- Encoder-Decoder  (e.g. BART or T5)\n",
    "  \n",
    "Nous pouvons t√©l√©charger notre mod√®le pr√©-entra√Æn√© de la m√™me mani√®re que nous l‚Äôavons fait avec notre tokenizer. Transformers fournit une classe AutoModel qui poss√®de √©galement une m√©thode from_pretrained().\n",
    "\n",
    "Attention, les `*Model' sont des mod√®les de base sans post-training (finetuning). Les autres mod√®les de type `ForSequenceClassification`, '`For...` PEUVENT AVOIR ETE Fietun√© Mais ce n'est pas toujours le cas. \n",
    "\n",
    "| Classe de mod√®le                          | Exemple(s) de mod√®le                                | Fine-tun√© ? | T√¢che cibl√©e                          |\n",
    "|-------------------------------------------|-----------------------------------------------------|-------------|----------------------------------------|\n",
    "| `AutoModel`                               | `bert-base-uncased`, `roberta-base`                  | Non      | Repr√©sentation de texte (pr√©-entra√Ænement) |\n",
    "| `AutoModelForCausalLM`                    | `gpt2`, `llama-7b-hf`                                |  Non      | Mod√®le g√©n√©ratif (langage auto-r√©gressif) |\n",
    "| `AutoModelForMaskedLM`                    | `bert-base-uncased`, `distilbert-base-uncased`       |  Non      | Pr√©diction de tokens masqu√©s (MLM)     |\n",
    "| `AutoModelForSequenceClassification`      | `distilbert-base-uncased-finetuned-sst-2-english`     | Oui      | Classification de texte                |\n",
    "| `AutoModelForTokenClassification`         | `dbmdz/bert-large-cased-finetuned-conll03-english`    | Oui      | Reconnaissance d'entit√©s nomm√©es (NER) |\n",
    "| `AutoModelForQuestionAnswering`           | `distilbert-base-uncased-distilled-squad`              | Oui      | Question Answering (Q&A)               |\n",
    "| `AutoModelForMultipleChoice`              | `bert-base-uncased`                                    |  Non\\*    | Doit √™tre fine-tun√© manuellement       |\n",
    "| `AutoModelForNextSentencePrediction`      | `bert-base-uncased`                                     | Non      | T√¢che de pr√©-entra√Ænement NSP          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model\n",
      "Loading classification model from base model's checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, DistilBertForSequenceClassification, DistilBertModel\n",
    "\n",
    "print('Loading base model')\n",
    "checkpoint='distilbert-base-cased'\n",
    "#checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "input_str = \"Using a Transformer network is simpled\"\n",
    "# le mod√®le de base sans aucune t√¢che \n",
    "#base_model = DistilBertModel.from_pretrained(checkpoint)\n",
    "# Mod√®les pr√©par√©e pour la t√¢che de classification mais pas finetun√©e\n",
    "print(\"Loading classification model from base model's checkpoint\")\n",
    "#model = DistilBertForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "print(model.config.num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")>\n",
      "Number of parameters : 65.783042\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters)\n",
    "print(\"Number of parameters :\", model.num_parameters() / 1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)  \n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "input_str = \"Using a Transformer network is simpled\"\n",
    "\n",
    "# Tokenize input string\n",
    "inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "\n",
    "# Pass to model\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Check output shape # Ecriture pytorch\n",
    "print(outputs.last_hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez acc√©der aux √©l√©ments par attributs (comme nous l‚Äôavons fait), par cl√© (outputs[\"last_hidden_state\"]), ou m√™me par l‚Äôindex si vous savez exactement o√π se trouve la chose que vous cherchez (outputs[0])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choix d'un mod√®le finetun√© "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "text = \"I love Hugging Face!\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Supprimer token_type_ids (non support√© par DistilBERT)\n",
    "#inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "# Appel du mod√®le\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Acc√®s aux logits\n",
    "print(outputs.logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dans le cas\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-traitement de la sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous pouvons voir que le mod√®le a pr√©dit [0.0402, 0.9598] . Ce sont des scores de probabilit√© reconnaissables.\n",
    "\n",
    "Pour obtenir les √©tiquettes correspondant √† chaque position, nous pouvons inspecter l‚Äôattribut id2label de la configuration du mod√®le (plus de d√©tails dans la section suivante) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G√©n√©ration de textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a Transformer network is simpled by the fact that the network is connected to the network.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "checkpoint=\"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)  \n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n",
    "input_str = \"Using a Transformer network is simpled\"\n",
    "\n",
    "# Tokenize input string\n",
    "inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "\n",
    "# Pass to model\n",
    "#outputs = model.generate(inputs.input_ids,max_length=10)\n",
    "\n",
    "outputs =model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_length=20,\n",
    "    pad_token_id=tokenizer.eos_token_id,   \n",
    "    attention_mask=inputs.attention_mask   \n",
    ")\n",
    "\n",
    "# Decode\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Derri√®re le pipeline (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
