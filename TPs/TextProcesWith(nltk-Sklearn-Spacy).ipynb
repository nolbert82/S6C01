{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apGb9jF0zCjf"
   },
   "source": [
    "# Traitement de textes : Text processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRmLJ2x8zCjg"
   },
   "source": [
    "Il existe différentes librairies sous Python pour analyser le texte. \n",
    "L'analyse peut aller de l'extraction de mots simple (une tokenisation), jusqu'à l'analyse un peu plus \"poussée\" </br>\n",
    "telle que la construction, de vecteurs de mots pondérés, de vecteurs continus (LSI, LDA, embedings,..).\n",
    "De manière générale le  \"text processing\" passe par les étapes suivantes=\n",
    "- segmentation des textes en mots simples (tokenizer) avec ou sans suppression de mots vides\n",
    "- construction du vocabulaire (le dictionnaire): vocabulary—lexicon\n",
    "- vectorisation d'un texte avec ou non pondération\n",
    "\n",
    "Il existe différentes librairies qui assurent ces fonctions d'analyse de textes: NTK, SKLEARN, GENSIM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYTxu35_zCjj",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1 - Bibliothèque (librairie) NLTK\n",
    "Le texte ci-dessous montre deux fonctions, \n",
    "- la première construit à partir d'un texte, son sac de mots (le texte en question est segmenté (split), tokenizé, suppression de mots vides, normalisation.\n",
    "- La seconde fonction fait un comptage (bag of words avec les fréquence des mots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4745,
     "status": "ok",
     "timestamp": 1642006294961,
     "user": {
      "displayName": "Mohand Boughanem",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjHAw9QreTcjTYo6HapvauQ1YxCB2g-roUErNg=s64",
      "userId": "11156155735843474973"
     },
     "user_tz": -60
    },
    "id": "JmDe35KkiC_f",
    "outputId": "0021325f-db95-43d3-b222-e59e530730e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1642006249977,
     "user": {
      "displayName": "Mohand Boughanem",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjHAw9QreTcjTYo6HapvauQ1YxCB2g-roUErNg=s64",
      "userId": "11156155735843474973"
     },
     "user_tz": -60
    },
    "id": "MZPuVnIYzCjj",
    "outputId": "7bc06210-e346-450a-f141-20d2b3c5353e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#NLTK\n",
    "import nltk\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize \n",
    "from nltk.util import ngrams\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture des données (Load text file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./data/text_file1.txt') as f:\n",
    "        text=f.read()\n",
    "\n",
    "text_fr='''ceci est un exemple de texte qui sera traité par nltK. \\\n",
    "      Il existe différentes librairies sous Python pour analyser le texte. \\\n",
    "      L'analyse peut consister à extraire les mots simples, jusqu''à l''analyse un peu plus \\\n",
    "      \"poussée\", vecteurs de mots pondérés, représentations continues (LSI, LDA, embedings,..).\\\n",
    "      '''\n",
    "\n",
    "text_en=\"    Text representation is one of the fundamental problems in text mining and \\\n",
    "    Information Retrieval (IR).\\\n",
    "    It aims to numerically represent the unstructured text documents to make them mathematically computable.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extraire les phrases\n",
    "Un text peut être segmenté de différentes manières, par phrase (sentence), par mots, par charactères, par n-grams, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences=sent_tokenize(text_fr)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxGFUbaZzCjk",
    "tags": []
   },
   "source": [
    "#### Extraire les mots (tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "executionInfo": {
     "elapsed": 679,
     "status": "error",
     "timestamp": 1694513960109,
     "user": {
      "displayName": "Mohand Boughanem",
      "userId": "11156155735843474973"
     },
     "user_tz": -120
    },
    "id": "NS7-Qcs_zCjm",
    "outputId": "65554a7f-fdf3-44ff-8fc7-2dd46a3cb439",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# seuls les mots (tokens) sont extraits)\n",
    "tokens = word_tokenize(text_fr.lower())\n",
    "print(\"LES TOKENS:\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraire les n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "executionInfo": {
     "elapsed": 679,
     "status": "error",
     "timestamp": 1694513960109,
     "user": {
      "displayName": "Mohand Boughanem",
      "userId": "11156155735843474973"
     },
     "user_tz": -120
    },
    "id": "NS7-Qcs_zCjm",
    "outputId": "65554a7f-fdf3-44ff-8fc7-2dd46a3cb439",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#récuperer la liste des n grammes (récupérer les mots adjenents 2 à deux.)\n",
    "print()\n",
    "list(ngrams(tokens,2))\n",
    "print('LES nGRAMs')\n",
    "print(list(ngrams(tokens,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Suppression des mots vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# suppression des stop words \"french, english, ...\n",
    "stop_words = nltk.corpus.stopwords.words('french')\n",
    "#Liste des mots sans les mots vides\n",
    "new_tokens = [w for w in tokens if not w in stop_words]\n",
    "print(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression des caractères spéciaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string \n",
    "cleaned_tokens = [token for token in new_tokens \n",
    "                if not token.isdigit() and not token in string.punctuation] \n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation\n",
    "- Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer,  SnowballStemmer\n",
    "\n",
    "#for french \n",
    "stemmer = SnowballStemmer(language='french')\n",
    "\n",
    "# English\n",
    "#stemmer = PorterStemmer()\n",
    "#stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "stemmed_tokens = [stemmer.stem(w) for w in new_tokens if not w in stop_words]\n",
    "print(\"Normalisation selon lemmatiseur\")\n",
    "print(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Lemmatisation (utiliser WoredNet par exemple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(w) for w in new_tokens if not w in stop_words]\n",
    "#print(\"Normalisation selon lemmatiseur\")\n",
    "print(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Comptage des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "executionInfo": {
     "elapsed": 679,
     "status": "error",
     "timestamp": 1694513960109,
     "user": {
      "displayName": "Mohand Boughanem",
      "userId": "11156155735843474973"
     },
     "user_tz": -120
    },
    "id": "NS7-Qcs_zCjm",
    "outputId": "65554a7f-fdf3-44ff-8fc7-2dd46a3cb439",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compter les tokens (construire le bag of words (Mot,count))\n",
    "# librairie pour le comptage des mots\n",
    "from collections import Counter\n",
    "bag_of_words = Counter(cleaned_tokens)\n",
    "print(\"\\nLES BAG OF WORDS\")\n",
    "print(bag_of_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2- Bibliothèque Spacy\n",
    "Cette bibliothèque offre plusieurs méthodes permettant de traiter de manière fine le texte.</br>\n",
    "Elle propose une méthode générique, nommée, nlp, qui prend en argument un pipepline (prédéfini) (vous pourrez aussi définir votre propre pipline)(https://spacy.io/models).</br>\n",
    "Le pipeline comporte un ensemble d'étapes de traitement d'un texte, qui va \n",
    "- du plus simple, nlp=spacy.bank(\"en\"), pipline comportant uniquement le tokeniser, ici english, pour le françcais 'fr\"\n",
    "- jusqu'au plus complet nlp=spacy(\"fr_core_news_sm\")( pour les textes en français), (en_core_web_sm pour l'anglais), qui propose plusieurs étapes (voir les cellules ci-dessous) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Pipeline spacy minimal:</br>\n",
    " <img height=300 width=400 src=\"./images/spacy_blank_pipeline.jpg\" />\n",
    "\n",
    "Pipeline spacy plus complet:</br>\n",
    "<img height=400 width=500 src=\"./images/image_spacy_pipeline.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline spacy minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# On commence par le modèle le plus simple\n",
    "nlp=spacy.blank(\"fr\")\n",
    "\n",
    "text_fr='''ceci est un exemple de texte qui sera traité par nltK. \\\n",
    "      Il existe différentes librairies sous Python pour analyser le texte. \\\n",
    "      L'analyse peut consister à extraire les mots simples, jusqu''à l''analyse un peu plus \\\n",
    "      \"poussée\", vecteurs de mots pondérés, représentations continues (LSI, LDA, embedings,..).\\\n",
    "      '''\n",
    "\n",
    "text_en=\"Text representation is one of the fundamental problems in text mining and Information Retrieval (IR).\\\n",
    "#It aims to numerically represent the unstructured text documents to make them mathematically computable\"\n",
    "\n",
    "text=text_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# traiter le texte\n",
    "doc=nlp(text)\n",
    "print (doc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Afficher les différents composantns NLP qui ont été utilisés dans ce pipline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline spacy prédéfini \n",
    "Vous pourrez aller sur le site de spacy pour sélectionner le pipeline de votre choix.\n",
    "\n",
    "<img height=400 width=500 src=\"./images/list_tasks_spacy.png\" />\n",
    "\n",
    "On peut prendre l'un de ces piplines </br>\n",
    "- fr_code_news_sm, pour traiter des textes en français\n",
    "- en_core_web_sm pour l'anglais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Un simple appel nlp\n",
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Afficher les composants du pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exemple affichage des token (les mots) du lemme et de la POS (etiquetage grammatical) des mots\n",
    "for token in doc:\n",
    "    print(token, \" | \", spacy.explain(token.pos_), \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dir(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Bibliothèque Sklearn\n",
    "Scikit learn (SKLEARN) est une bibliothèque de \"Machine learning\". </br>\n",
    "[Scikit-learn](https://scikit-learn.org/stable/index.html)\n",
    "Elle propose deux manières (méthodes) assez simples pour construire des vecteurs de textes :\n",
    "- Un simple `countVectorizer` qui construit un vecteur d'id (idenifiant de tokens)avec les férquences des mots \n",
    "- ou un `TfidfVectorizer` vecteur, IDs pondérés à la tf.idf\n",
    "\n",
    "[Scikit-learn- vectorization](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text)\n",
    "Vous trouverez ci dessous ces deux cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hhd2uF0GzCjo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#NLP avec scikit-learn\n",
    "# Libraire pour la vectorization des textes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lecture des données.\n",
    "Vous pourrez manipuler deux types de textes (documents). Le premier très petit  5 documents, définis dans une liste python, </br>\n",
    "le second est un fichier comportant plus de 800000 documents issus du web (on ne va pas travailler sur l'ensemble des documents de ce fichier).</br>\n",
    "La fonction load permet de limiter le nombre de documents à extraire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcjWFbWCzCjp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choisir une option lecture du Texte et la procédure de construction des tockens\n",
    "\n",
    "# Sample of 5 documents \n",
    "def get_documents():\n",
    "    documents = [\n",
    "    \"Text retrieval is the process of finding text and documents, that are relevant to a user's query.\",\n",
    "    \"Gensim is a popular library for topic modeling and text retrieval in Python.\",\n",
    "    \"Cosine similarity is a metric used to measure how similar two vectors are.\",\n",
    "    \"Vectorization is the process of converting text data into numerical vectors.\",\n",
    "    \"Python is a versatile programming language used for various applications.\"\n",
    "    ]  \n",
    "    return documents\n",
    "\n",
    "## Plusieurs fichiers dans un répértoire\n",
    "def readfiles_from_dir(dir_path='./data'):\n",
    "    for file_name in os.listdir(dir_path):\n",
    "        if \".txt\" in file_name:\n",
    "            texts = [simple_preprocess(remove_stopwords(sentence))\n",
    "                  for sentence in open(os.path.join(dir_path, file_name), encoding='utf-8')]\n",
    "    return texts\n",
    "\n",
    "# Function to read texts from a file\n",
    "def read_texts_from_file(file_path,n):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        texts = file.readlines()\n",
    "\n",
    "    return texts[0:n]\n",
    "\n",
    "def get_names():\n",
    "    documents = [\n",
    "    \"Dupont\",\n",
    "    \"dupond\",\n",
    "    \"martin\"\n",
    "    ]  \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lecture du fichier \n",
    "#file_path = './data/msmarco/collection.tsv'\n",
    "##documents = read_texts_from_file(file_path,100)\n",
    "#file_path='../cours/pdf/Cours_RI.pdf'\n",
    "#documents=get_pdf_doc(file_path)\n",
    "\n",
    "documents=get_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "names=get_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convertir les documents sous forme vectorielle\n",
    "- Un vecteur par document comportant TfDdf (ou un simple count) \n",
    "- On peut limiter le nombre de termes de l'espace vectoriel, ici à 20000 par exemple</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Un simple counter \n",
    "#(le résultat est une matrice TermexDocuments avec la fréquence de chaque terme dans un document\n",
    "# Définir un modèle de trasformation \n",
    "cv = CountVectorizer(analyzer='word', ngram_range = (1, 1), max_features=100)\n",
    "\n",
    "doc_vectors_count = cv.fit_transform(documents).todense()\n",
    "\n",
    "# on peut aussi extraire des n-grams \n",
    "#il suffit de rajouter ngram_range = (2, 3) dans les paramètres (extraire les bigrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 1, 0, 0, 1, 1, 1, 0, 0, 2, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectors_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut aussi extraire des n-grams \n",
    "#il suffit de rajouter ngram_range = (2, 3) (bi et tri grams) dans les paramètres (extraire les bigrams)\n",
    "\n",
    "cv = CountVectorizer(analyzer='word', ngram_range = (1, 2), max_features=100)\n",
    "\n",
    "doc_vectors_count = cv.fit_transform(documents).todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 65,\n",
       " 'retrieval': 58,\n",
       " 'is': 27,\n",
       " 'the': 71,\n",
       " 'process': 49,\n",
       " 'of': 44,\n",
       " 'finding': 14,\n",
       " 'and': 0,\n",
       " 'documents': 12,\n",
       " 'that': 69,\n",
       " 'are': 4,\n",
       " 'relevant': 56,\n",
       " 'to': 73,\n",
       " 'user': 83,\n",
       " 'query': 55,\n",
       " 'text retrieval': 68,\n",
       " 'retrieval is': 60,\n",
       " 'is the': 30,\n",
       " 'the process': 72,\n",
       " 'process of': 50,\n",
       " 'of finding': 46,\n",
       " 'finding text': 15,\n",
       " 'text and': 66,\n",
       " 'and documents': 1,\n",
       " 'documents that': 13,\n",
       " 'that are': 70,\n",
       " 'are relevant': 5,\n",
       " 'relevant to': 57,\n",
       " 'to user': 75,\n",
       " 'user query': 84,\n",
       " 'gensim': 19,\n",
       " 'popular': 47,\n",
       " 'library': 34,\n",
       " 'for': 16,\n",
       " 'topic': 76,\n",
       " 'modeling': 40,\n",
       " 'in': 23,\n",
       " 'python': 53,\n",
       " 'gensim is': 20,\n",
       " 'is popular': 29,\n",
       " 'popular library': 48,\n",
       " 'library for': 35,\n",
       " 'for topic': 17,\n",
       " 'topic modeling': 77,\n",
       " 'modeling and': 41,\n",
       " 'and text': 2,\n",
       " 'retrieval in': 59,\n",
       " 'in python': 24,\n",
       " 'cosine': 8,\n",
       " 'similarity': 63,\n",
       " 'metric': 38,\n",
       " 'used': 80,\n",
       " 'measure': 36,\n",
       " 'how': 21,\n",
       " 'similar': 61,\n",
       " 'two': 78,\n",
       " 'vectors': 89,\n",
       " 'cosine similarity': 9,\n",
       " 'similarity is': 64,\n",
       " 'is metric': 28,\n",
       " 'metric used': 39,\n",
       " 'used to': 82,\n",
       " 'to measure': 74,\n",
       " 'measure how': 37,\n",
       " 'how similar': 22,\n",
       " 'similar two': 62,\n",
       " 'two vectors': 79,\n",
       " 'vectors are': 90,\n",
       " 'vectorization': 87,\n",
       " 'converting': 6,\n",
       " 'data': 10,\n",
       " 'into': 25,\n",
       " 'numerical': 42,\n",
       " 'vectorization is': 88,\n",
       " 'of converting': 45,\n",
       " 'converting text': 7,\n",
       " 'text data': 67,\n",
       " 'data into': 11,\n",
       " 'into numerical': 26,\n",
       " 'numerical vectors': 43,\n",
       " 'versatile': 91,\n",
       " 'programming': 51,\n",
       " 'language': 32,\n",
       " 'various': 85,\n",
       " 'applications': 3,\n",
       " 'python is': 54,\n",
       " 'is versatile': 31,\n",
       " 'versatile programming': 92,\n",
       " 'programming language': 52,\n",
       " 'language used': 33,\n",
       " 'used for': 81,\n",
       " 'for various': 18,\n",
       " 'various applications': 86}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#afficher le vocabulaire\n",
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#afficher (la liste des features (le vocabulaire) non traité\"\n",
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 2 1 0 1 1 1 1\n",
      "  1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# afficher le vecteur du premier document \n",
    "print(doc_vectors_count[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#afficher les dimensions de la matrice doc-termes(voca)\n",
    "doc_vectors_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Une version avec tfidf\n",
    "tf_idf_vectorizer = TfidfVectorizer(stop_words='english', token_pattern=r'\\w+', max_features=100)\n",
    "doc_vectors_tfidf = tf_idf_vectorizer.fit_transform(documents)\n",
    "#.todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualiser (le vocabulaire, les vecteurs des documents, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.3983516165374428\n",
      "  (0, 27)\t0.3983516165374428\n",
      "  (0, 7)\t0.3983516165374428\n",
      "  (0, 15)\t0.3983516165374428\n",
      "  (0, 30)\t0.3983516165374428\n",
      "  (0, 25)\t0.32138757599667\n",
      "  (0, 16)\t0.32138757599667\n"
     ]
    }
   ],
   "source": [
    "print(doc_vectors_tfidf[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXQZGjE7zCjo"
   },
   "source": [
    "#### Calcul des similarites entre textes\n",
    "- Calculer la simialrité entre deux textes\n",
    "- Vérifier les dimensions des vecteurs requête et documents.</br>\n",
    "[Sklearn cosine-similarity](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1694514138587,
     "user": {
      "displayName": "Mohand Boughanem",
      "userId": "11156155735843474973"
     },
     "user_tz": -120
    },
    "id": "-Wi66QdczCjp",
    "outputId": "ead359e0-a37f-4907-f313-7dddc2588c02",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#User query\n",
    "query = \"text retrieval france\"\n",
    "# transform la requête sous forme d'un vecteur dans l'espace engendré par le vocavulaire des documents\n",
    "# on utlise un ransform sans le fit  \n",
    "query_vector = tf_idf_vectorizer.transform([query])\n",
    "\n",
    "# Calculate cosine similarity between the query and documents\n",
    "similarities = cosine_similarity(query_vector, doc_vectors_tfidf).flatten()\n",
    "\n",
    "# Get the index of the most similar document\n",
    "most_similar_index = np.argmax(similarities)\n",
    "\n",
    "# Display the most relevant document\n",
    "print(\"Query:\", query)\n",
    "print(\"Most relevant document:\", documents[most_similar_index])\n",
    "print(\"Cosine Similarity:\", similarities[most_similar_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(query_vector.shape, doc_vectors_tfidf.shape,similarities.flatten().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tfidf_top_features(documents,n_top=10):\n",
    "  tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,  stop_words='english')\n",
    "  tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "  importance = np.argsort(np.asarray(tfidf.sum(axis=0)).ravel())[::-1]\n",
    "  tfidf_feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "  return tfidf_feature_names[importance[:n_top]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_tfidf_top_features(documents)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
